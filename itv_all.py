import time
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import requests
import re
import os
import threading
from queue import Queue
import eventlet
import base64
import random
from fake_useragent import UserAgent
import json
from datetime import datetime

eventlet.monkey_patch()

# é…ç½®åŒºåŸŸ - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
CONFIG = {
    # FOFA APIé…ç½®ï¼ˆæ¨èä½¿ç”¨ï¼‰
    "fofa_email": "your_email@example.com",  # æ›¿æ¢ä¸ºæ‚¨çš„FOFAé‚®ç®±
    "fofa_key": "your_api_key_here",         # æ›¿æ¢ä¸ºæ‚¨çš„FOFA APIå¯†é’¥
    
    # åœ°åŒºæœç´¢è¯ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    "regions": {
        "hebei": '"iptv/live/zh_cn.js" && country="CN" && region="æ²³åŒ—"',
        "beijing": '"iptv/live/zh_cn.js" && country="CN" && region="åŒ—äº¬"',
        "guangdong": '"iptv/live/zh_cn.js" && country="CN" && region="å¹¿ä¸œ"',
        "shanghai": '"iptv/live/zh_cn.js" && country="CN" && region="ä¸Šæµ·"',
        "tianjin": '"iptv/live/zh_cn.js" && country="CN" && region="å¤©æ´¥"',
        "chongqing": '"iptv/live/zh_cn.js" && country="CN" && region="é‡åº†"',
        "shanxi": '"iptv/live/zh_cn.js" && country="CN" && region="å±±è¥¿"',
        "shaanxi": '"iptv/live/zh_cn.js" && country="CN" && region="é™•è¥¿"',
        "liaoning": '"iptv/live/zh_cn.js" && country="CN" && region="è¾½å®"',
        "jiangsu": '"iptv/live/zh_cn.js" && country="CN" && region="æ±Ÿè‹"',
        "zhejiang": '"iptv/live/zh_cn.js" && country="CN" && region="æµ™æ±Ÿ"',
        "anhui": '"iptv/live/zh_cn.js" && country="CN" && region="å®‰å¾½"',
        "fujian": '"iptv/live/zh_cn.js" && country="CN" && region="ç¦å»º"',
        "jiangxi": '"iptv/live/zh_cn.js" && country="CN" && region="æ±Ÿè¥¿"',
        "shandong": '"iptv/live/zh_cn.js" && country="CN" && region="å±±ä¸œ"',
        "henan": '"iptv/live/zh_cn.js" && country="CN" && region="æ²³å—"',
        "hubei": '"iptv/live/zh_cn.js" && country="CN" && region="æ¹–åŒ—"',
        "hunan": '"iptv/live/zh_cn.js" && country="CN" && region="æ¹–å—"'
    },
    
    # è¯·æ±‚è®¾ç½®
    "timeout": 3,           # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "max_workers": 50,      # æœ€å¤§çº¿ç¨‹æ•°
    "max_retries": 3,       # æœ€å¤§é‡è¯•æ¬¡æ•°
    
    # é¢‘é“è®¾ç½®
    "result_counter": 8,    # æ¯ä¸ªé¢‘é“ä¿ç•™çš„æœ€å¤§æ•°é‡
    "min_speed": 0.1,       # æœ€ä½é€Ÿåº¦è¦æ±‚ï¼ˆMB/sï¼‰
}

class SecureFOFACrawler:
    def __init__(self):
        self.ua = UserAgent()
        self.results = []
        self.channels = []
        self.error_channels = []
        
    def search_fofa_api(self, query, page=1, size=100):
        """ä½¿ç”¨FOFAå®˜æ–¹APIæœç´¢"""
        if not CONFIG["fofa_email"] or not CONFIG["fofa_key"]:
            print("âš ï¸ è­¦å‘Š: æœªé…ç½®FOFA APIå¯†é’¥ï¼Œå°†å°è¯•ä½¿ç”¨çˆ¬å–æ–¹å¼")
            return []
            
        try:
            query_base64 = base64.b64encode(query.encode()).decode()
            api_url = "https://fofa.info/api/v1/search/all"
            params = {
                'email': CONFIG["fofa_email"],
                'key': CONFIG["fofa_key"],
                'qbase64': query_base64,
                'page': page,
                'size': size,
                'fields': 'ip,port,protocol,host'
            }
            
            response = requests.get(api_url, params=params, timeout=CONFIG["timeout"])
            if response.status_code == 200:
                data = response.json()
                if data.get('error'):
                    print(f"âŒ APIé”™è¯¯: {data.get('errmsg', 'æœªçŸ¥é”™è¯¯')}")
                    return []
                return data.get('results', [])
        except Exception as e:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
            
        return []
    
    def create_stealth_driver(self):
        """åˆ›å»ºéšå½¢çš„æµè§ˆå™¨å®ä¾‹"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument(f'--user-agent={self.ua.random}')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_exmental_option('useAutomationExtension', False)
        
        driver = webdriver.Chrome(options=chrome_options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver
    
    def crawl_fofa(self, url, max_retries=3):
        """çˆ¬å–FOFAæœç´¢ç»“æœ"""
        for attempt in range(max_retries):
            try:
                driver = self.create_stealth_driver()
                
                # éšæœºå»¶è¿Ÿ
                time.sleep(random.uniform(5, 15))
                
                driver.get(url)
                
                # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                self.simulate_human_behavior(driver)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                time.sleep(random.uniform(8, 15))
                
                page_content = driver.page_source
                driver.quit()
                
                # æ£€æŸ¥æ˜¯å¦è¢«å°ç¦
                if "IPè®¿é—®å¼‚å¸¸" in page_content or "çˆ¬è™«" in page_content:
                    print(f"âŒ ç¬¬{attempt+1}æ¬¡å°è¯•è¢«æ£€æµ‹ä¸ºçˆ¬è™«")
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 60
                        print(f"â³ ç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                        time.sleep(wait_time)
                    continue
                    
                return page_content
                
            except Exception as e:
                print(f"âŒ ç¬¬{attempt+1}æ¬¡çˆ¬å–å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(30)
                    
        return None
    
    def simulate_human_behavior(self, driver):
        """æ¨¡æ‹Ÿäººç±»æµè§ˆè¡Œä¸º"""
        # éšæœºæ»šåŠ¨é¡µé¢
        scroll_actions = [
            "window.scrollTo(0, document.body.scrollHeight * 0.3);",
            "window.scrollTo(0, document.body.scrollHeight * 0.7);", 
            "window.scrollTo(0, document.body.scrollHeight);"
        ]
        
        for action in scroll_actions:
            driver.execute_script(action)
            time.sleep(random.uniform(1, 3))
    
    def extract_ips_from_page(self, page_content):
        """ä»é¡µé¢å†…å®¹æå–IPåœ°å€"""
        pattern = r"http://\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}:\d+"
        urls_all = re.findall(pattern, page_content)
        urls = set(urls_all)
        
        # å¤„ç†IPï¼Œå°†ç¬¬å››ä½æ”¹ä¸º1
        processed_urls = []
        for url in urls:
            url = url.strip()
            ip_start_index = url.find("//") + 2
            ip_end_index = url.find(":", ip_start_index)
            ip_dot_start = url.find(".") + 1
            ip_dot_second = url.find(".", ip_dot_start) + 1
            ip_dot_three = url.find(".", ip_dot_second) + 1
            base_url = url[:ip_start_index]
            ip_address = url[ip_start_index:ip_dot_three]
            port = url[ip_end_index:]
            modified_ip = f"{ip_address}1"
            processed_url = f"{base_url}{modified_ip}{port}"
            processed_urls.append(processed_url)
            
        return set(processed_urls)
    
    def extract_ips_from_api(self, api_results):
        """ä»APIç»“æœæå–IPåœ°å€"""
        processed_urls = []
        for result in api_results:
            ip = result[0]
            port = result[1]
            protocol = result[2].lower() if len(result) > 2 else "http"
            processed_url = f"{protocol}://{ip[:-1]}1:{port}"
            processed_urls.append(processed_url)
            
        return set(processed_urls)
    
    def modify_urls(self, url):
        """ç”Ÿæˆæµ‹è¯•URL"""
        modified_urls = []
        ip_start_index = url.find("//") + 2
        ip_end_index = url.find(":", ip_start_index)
        base_url = url[:ip_start_index]
        ip_address = url[ip_start_index:ip_end_index]
        port = url[ip_end_index:]
        ip_end = "/iptv/live/1000.json?key=txiptv"
        
        for i in range(1, 256):
            modified_ip = f"{ip_address[:-1]}{i}"
            modified_url = f"{base_url}{modified_ip}{port}{ip_end}"
            modified_urls.append(modified_url)
            
        return modified_urls
    
    def is_url_accessible(self, url):
        """æ£€æŸ¥URLæ˜¯å¦å¯è®¿é—®"""
        for attempt in range(CONFIG["max_retries"]):
            try:
                response = requests.get(url, timeout=CONFIG["timeout"])
                if response.status_code == 200:
                    return url
            except:
                if attempt < CONFIG["max_retries"] - 1:
                    time.sleep(1)
        return None
    
    def fetch_all_ips(self):
        """è·å–æ‰€æœ‰IPåœ°å€"""
        all_ips = set()
        
        for region, query in CONFIG["regions"].items():
            print(f"ğŸ” æœç´¢åœ°åŒº: {region}")
            
            # ä¼˜å…ˆä½¿ç”¨API
            api_results = self.search_fofa_api(query)
            if api_results:
                ips = self.extract_ips_from_api(api_results)
                all_ips.update(ips)
                print(f"âœ… é€šè¿‡APIæ‰¾åˆ° {len(ips)} ä¸ªIP")
                continue
                
            # APIå¤±è´¥æ—¶ä½¿ç”¨çˆ¬å–
            query_base64 = base64.b64encode(query.encode()).decode()
            fofa_url = f"https://fofa.info/result?qbase64={query_base64}"
            
            page_content = self.crawl_fofa(fofa_url)
            if page_content:
                ips = self.extract_ips_from_page(page_content)
                all_ips.update(ips)
                print(f"âœ… é€šè¿‡çˆ¬å–æ‰¾åˆ° {len(ips)} ä¸ªIP")
            else:
                print(f"âŒ æ— æ³•è·å– {region} çš„IP")
                
        return all_ips
    
    def test_urls(self, urls):
        """æµ‹è¯•URLå¯ç”¨æ€§"""
        valid_urls = []
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=CONFIG["max_workers"]) as executor:
            futures = []
            for url in urls:
                modified_urls = self.modify_urls(url)
                for modified_url in modified_urls:
                    futures.append(executor.submit(self.is_url_accessible, modified_url))
            
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if result:
                    valid_urls.append(result)
                    print(f"âœ… å¯ç”¨URL: {result}")
        
        return valid_urls
    
    def parse_json_data(self, url):
        """è§£æJSONæ•°æ®è·å–é¢‘é“ä¿¡æ¯"""
        try:
            ip_start_index = url.find("//") + 2
            ip_dot_start = url.find(".") + 1
            ip_index_second = url.find("/", ip_dot_start)
            base_url = url[:ip_start_index]
            ip_address = url[ip_start_index:ip_index_second]
            url_x = f"{base_url}{ip_address}"
            
            response = requests.get(url, timeout=CONFIG["timeout"])
            json_data = response.json()
            
            channels = []
            for item in json_data['data']:
                if isinstance(item, dict):
                    name = item.get('name')
                    urlx = item.get('url')
                    
                    if not name or not urlx:
                        continue
                    
                    if ',' in urlx:
                        continue
                        
                    if 'http' in urlx:
                        urld = f"{urlx}"
                    else:
                        urld = f"{url_x}{urlx}"
                    
                    # æ¸…ç†é¢‘é“åç§°
                    name = self.clean_channel_name(name)
                    channels.append((name, urld))
            
            return channels
            
        except Exception as e:
            print(f"âŒ è§£æJSONå¤±è´¥: {e}")
            return []
    
    def clean_channel_name(self, name):
        """æ¸…ç†é¢‘é“åç§°"""
        replacements = {
            "cctv": "CCTV",
            "ä¸­å¤®": "CCTV",
            "å¤®è§†": "CCTV",
            "é«˜æ¸…": "",
            "è¶…é«˜": "",
            "HD": "",
            "æ ‡æ¸…": "",
            "é¢‘é“": "",
            "-": "",
            " ": "",
            "PLUS": "+",
            "ï¼‹": "+",
            "(": "",
            ")": "",
            "CCTV1ç»¼åˆ": "CCTV1",
            "CCTV2è´¢ç»": "CCTV2",
            "CCTV3ç»¼è‰º": "CCTV3",
            "CCTV4å›½é™…": "CCTV4",
            "CCTV4ä¸­æ–‡å›½é™…": "CCTV4",
            "CCTV4æ¬§æ´²": "CCTV4",
            "CCTV5ä½“è‚²": "CCTV5",
            "CCTV6ç”µå½±": "CCTV6",
            "CCTV7å†›äº‹": "CCTV7",
            "CCTV7å†›å†œ": "CCTV7",
            "CCTV7å†œä¸š": "CCTV7",
            "CCTV7å›½é˜²å†›äº‹": "CCTV7",
            "CCTV8ç”µè§†å‰§": "CCTV8",
            "CCTV9è®°å½•": "CCTV9",
            "CCTV9çºªå½•": "CCTV9",
            "CCTV10ç§‘æ•™": "CCTV10",
            "CCTV11æˆæ›²": "CCTV11",
            "CCTV12ç¤¾ä¼šä¸æ³•": "CCTV12",
            "CCTV13æ–°é—»": "CCTV13",
            "CCTVæ–°é—»": "CCTV13",
            "CCTV14å°‘å„¿": "CCTV14",
            "CCTV15éŸ³ä¹": "CCTV15",
            "CCTV16å¥¥æ—åŒ¹å…‹": "CCTV16",
            "CCTV17å†œä¸šå†œæ‘": "CCTV17",
            "CCTV17å†œä¸š": "CCTV17",
            "CCTV5+ä½“è‚²èµ›è§†": "CCTV5+",
            "CCTV5+ä½“è‚²èµ›äº‹": "CCTV5+",
            "CCTV5+ä½“è‚²": "CCTV5+"
        }
        
        for old, new in replacements.items():
            name = name.replace(old, new)
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç†æ¨¡å¼åŒ¹é…
        name = re.sub(r"CCTV(\d+)å°", r"CCTV\1", name)
        
        return name
    
    def test_channel_speed(self, channel_name, channel_url):
        """æµ‹è¯•é¢‘é“é€Ÿåº¦"""
        try:
            channel_url_t = channel_url.rstrip(channel_url.split('/')[-1])
            response = requests.get(channel_url, timeout=CONFIG["timeout"])
            lines = response.text.strip().split('\n')
            ts_lists = [line.split('/')[-1] for line in lines if not line.startswith('#')]
            
            if not ts_lists:
                return None
                
            ts_lists_0 = ts_lists[0].rstrip(ts_lists[0].split('.ts')[-1])
            ts_url = channel_url_t + ts_lists[0]
            
            # ä½¿ç”¨eventletè®¾ç½®è¶…æ—¶
            with eventlet.Timeout(5, False):
                start_time = time.time()
                content = requests.get(ts_url, timeout=CONFIG["timeout"]).content
                end_time = time.time()
                
                if content:
                    file_size = len(content)
                    download_speed = file_size / (end_time - start_time) / 1024 / 1024  # MB/s
                    
                    if download_speed >= CONFIG["min_speed"]:
                        return channel_name, channel_url, f"{download_speed:.3f} MB/s"
        
        except:
            pass
            
        return None
    
    def worker(self):
        """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
        while True:
            channel_name, channel_url = self.task_queue.get()
            if channel_name is None:  # ç»ˆæ­¢ä¿¡å·
                break
                
            try:
                result = self.test_channel_speed(channel_name, channel_url)
                if result:
                    self.results.append(result)
                else:
                    self.error_channels.append((channel_name, channel_url))
            except Exception as e:
                self.error_channels.append((channel_name, channel_url))
                
            # æ›´æ–°è¿›åº¦
            processed = len(self.results) + len(self.error_channels)
            total = len(self.channels)
            percentage = (processed / total) * 100 if total > 0 else 0
            print(f"ğŸ“Š è¿›åº¦: {processed}/{total} ({percentage:.1f}%)")
            
            self.task_queue.task_done()
    
    def test_all_channels(self):
        """æµ‹è¯•æ‰€æœ‰é¢‘é“é€Ÿåº¦"""
        print("ğŸš€ å¼€å§‹æµ‹è¯•é¢‘é“é€Ÿåº¦...")
        
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        self.task_queue = Queue()
        for channel in self.channels:
            self.task_queue.put(channel)
        
        # åˆ›å»ºå·¥ä½œçº¿ç¨‹
        threads = []
        for _ in range(CONFIG["max_workers"]):
            t = threading.Thread(target=self.worker)
            t.daemon = True
            t.start()
            threads.append(t)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        self.task_queue.join()
        
        # å‘é€ç»ˆæ­¢ä¿¡å·
        for _ in range(CONFIG["max_workers"]):
            self.task_queue.put((None, None))
        
        for t in threads:
            t.join()
    
    def generate_playlist(self):
        """ç”Ÿæˆæ’­æ”¾åˆ—è¡¨"""
        print("ğŸ“ ç”Ÿæˆæ’­æ”¾åˆ—è¡¨...")
        
        # å¯¹é¢‘é“è¿›è¡Œæ’åº
        def channel_key(channel_name):
            match = re.search(r'\d+', channel_name)
            if match:
                return int(match.group())
            else:
                return float('inf')
        
        self.results.sort(key=lambda x: (x[0], -float(x[2].split()[0])))
        self.results.sort(key=lambda x: channel_key(x[0]))
        
        # ç”Ÿæˆitvlist.txt
        with open("itvlist.txt", 'w', encoding='utf-8') as file:
            file.write('å¤®è§†é¢‘é“,#genre#\n')
            channel_counters = {}
            for result in self.results:
                channel_name, channel_url, speed = result
                if 'CCTV' in channel_name:
                    if channel_name not in channel_counters:
                        channel_counters[channel_name] = 0
                    if channel_counters[channel_name] < CONFIG["result_counter"]:
                        file.write(f"{channel_name},{channel_url}\n")
                        channel_counters[channel_name] +=
